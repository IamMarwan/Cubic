{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f578c8b1-cd16-4b0d-b6de-080e006dcfb3",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93b84fa-1067-4416-8b5f-4c66cae6f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_root = Path().resolve().parents[0]\n",
    "src_path = project_root / \"src\"\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "load_dotenv(dotenv_path=project_root / \".env\")\n",
    "\n",
    "from rag_pipeline import answer_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa1b1e-fd7d-4867-bea8-78b0565fe895",
   "metadata": {},
   "source": [
    "**Load evaluation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d437c8-f64b-490e-bd15-72b617937e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,\n",
       " {'id': 'Q01',\n",
       "  'question': 'What city and country is Cubic Engineering Consultancy based in?',\n",
       "  'expected_source': 'cec_overview',\n",
       "  'notes': 'basic company location'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_dir = project_root / \"docs\"\n",
    "dataset_path = docs_dir / \"week2_eval_dataset.csv\"\n",
    "\n",
    "questions = []\n",
    "with dataset_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        questions.append(row)\n",
    "\n",
    "len(questions), questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7a7d0-55ec-4833-9f77-3209d47186b2",
   "metadata": {},
   "source": [
    "**Run evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da383a86-6452-424a-8c0d-59c71dafa652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Q01: What city and country is Cubic Engineering Consultancy based in?\n",
      "Running Q02: Name two stages of the project lifecycle that CEC supports.\n",
      "Running Q03: What is the main goal of the internal RAG system at Cubic?\n",
      "Running Q04: How many main steps does the internal RAG pipeline use\n",
      "Running Q05: Which embedding model is currently used in the RAG prototype?\n",
      "Running Q06: Which generation model is used to produce answers in the RAG system?\n",
      "Running Q07: How many top documents are retrieved by default for each question?\n",
      "Running Q08: What happens if none of the retrieved documents meet the minimum similarity threshold?\n",
      "Running Q09: What is the main role of LLM assistants at CEC with respect to final engineering decisions?\n",
      "Running Q10: Which values does CEC emphasize as core values in its work?\n",
      "Running Q11: What are two example tasks the AI assistant supports engineers with?\n",
      "Running Q12: Why is it important that the assistant uses only the retrieved context when answering?\n",
      "Running Q13: What should the assistant do when it does not have enough information to answer a question?\n",
      "Running Q14: Give an example of the safe fallback message recommended in the policies.\n",
      "Running Q15: Should the assistant invent project details or commitments if they are missing from the documents?\n",
      "Running Q16: What kind of questions should the assistant avoid answering because they are outside the knowledge base?\n",
      "Running Q17: Whose responsibility is it to approve important technical and design decisions: the assistant or human engineers?\n",
      "Running Q18: In the RAG pipeline\n",
      "Running Q19: What similarity measure is used to compare document embeddings with the question embedding?\n",
      "Running Q20: Is the current RAG prototype at CEC intended as a final decision-making tool?\n",
      "Running Q21: According to the overview\n",
      "Running Q22: What is one benefit of using LLMs mentioned in the overview of CEC?\n",
      "Running Q23: When the assistant answers\n",
      "Running Q24: If a user asks a personal question unrelated to CEC\n",
      "Running Q25: Why does the system use a minimum similarity threshold before answering?\n",
      "Running Q26: What type of documents are stored as the knowledge base for the internal RAG system?\n",
      "Running Q27: What is one of the main engineering focuses of CEC according to the overview?\n",
      "Running Q28: What two qualities does CEC want to maintain in its work besides solving challenges?\n",
      "Running Q29: What should the assistant recommend if it cannot answer due to missing information?\n",
      "Running Q30: In one sentence\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for row in questions:\n",
    "    qid = row[\"id\"]\n",
    "    question = row[\"question\"]\n",
    "    expected = row[\"expected_source\"]\n",
    "\n",
    "    print(f\"Running {qid}: {question}\")\n",
    "    result = answer_question(question)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": qid,\n",
    "            \"question\": question,\n",
    "            \"expected_source\": expected,\n",
    "            \"ok\": result[\"ok\"],\n",
    "            \"reason\": result[\"reason\"],\n",
    "            \"sources\": result.get(\"sources\", []),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557bdcd-2e53-43e6-8e92-327270d132de",
   "metadata": {},
   "source": [
    "**Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ededd69-b42a-43a8-9d8e-e9b917b7634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 30\n",
      "Correctly grounded answers: 17\n",
      "Safe refusals: 3\n",
      "Failures: 10\n"
     ]
    }
   ],
   "source": [
    "total = len(results)\n",
    "correct = 0\n",
    "safe_refusal = 0\n",
    "failures = []\n",
    "\n",
    "for r in results:\n",
    "    if not r[\"ok\"] and r[\"reason\"] == \"no_relevant_context\":\n",
    "        safe_refusal += 1\n",
    "    elif r[\"expected_source\"] in r[\"sources\"]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        failures.append(r)\n",
    "\n",
    "print(f\"Total questions: {total}\")\n",
    "print(f\"Correctly grounded answers: {correct}\")\n",
    "print(f\"Safe refusals: {safe_refusal}\")\n",
    "print(f\"Failures: {len(failures)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b3bbbc-ac73-481f-b1d4-4ac9863be32c",
   "metadata": {},
   "source": [
    "**Failure examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbce48d-8d3b-4f84-824b-631171bea498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Q04',\n",
       "  'question': 'How many main steps does the internal RAG pipeline use',\n",
       "  'expected_source': ' according to the documentation?',\n",
       "  'ok': True,\n",
       "  'reason': 'answered',\n",
       "  'sources': ['rag_overview']},\n",
       " {'id': 'Q12',\n",
       "  'question': 'Why is it important that the assistant uses only the retrieved context when answering?',\n",
       "  'expected_source': 'llm_policies',\n",
       "  'ok': True,\n",
       "  'reason': 'answered',\n",
       "  'sources': ['rag_overview']},\n",
       " {'id': 'Q13',\n",
       "  'question': 'What should the assistant do when it does not have enough information to answer a question?',\n",
       "  'expected_source': 'llm_policies',\n",
       "  'ok': True,\n",
       "  'reason': 'answered',\n",
       "  'sources': ['rag_overview']}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debed8e9-ef11-46eb-800f-eae6c39a1701",
   "metadata": {},
   "source": [
    "**Demo refusal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5137980b-aa73-47fd-ae9c-188c6d475543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': False,\n",
       " 'reason': 'no_relevant_context',\n",
       " 'answer': 'I’m not able to answer this question based on the available documents. Please consult a human engineer or update the knowledge base.',\n",
       " 'sources': [],\n",
       " 'retrieved': [{'doc_id': 'cec_overview',\n",
       "   'text': '# Cubic Engineering Consultancy – Overview\\n\\nCubic Engineering Consultancy (CEC) is an engineering and architectural consultancy based in Dubai, United Arab Emirates. CEC supports clients across the full project lifecycle, including design, feasibility studies, planning, project management and construction supervision.\\n\\nCEC focuses on solving complex engineering challenges, maintaining high quality standards, and building long-term partnerships with its clients. The company emphasizes honesty, responsibility, transparency and customer care as core values.\\n\\nIn internal LLM and RAG experiments, CEC uses AI assistants to help engineers navigate documentation, summarize technical notes and speed up access to project information. However, final design decisions are always made by qualified human engineers.',\n",
       "   'score': 0.018829916116235853},\n",
       "  {'doc_id': 'rag_overview',\n",
       "   'text': '# Internal RAG System – Overview\\n\\nThe internal Retrieval-Augmented Generation (RAG) system at Cubic Engineering Consultancy is designed to help engineers quickly access information from a curated knowledge base. The system works in three main steps:\\n\\n1. Documents are stored as text files and converted into vector embeddings using an OpenAI embedding model.\\n2. For each user question, the system computes an embedding and retrieves the most similar documents based on cosine similarity.\\n3. The retrieved documents are passed to a language model which generates an answer strictly using the provided context.\\n\\nThe default configuration retrieves the top 3 most relevant documents. If none of the documents reach a minimum similarity threshold, the system should not answer the question and must instead return a safe fallback message.\\n\\nThe current RAG prototype uses the `text-embedding-3-small` model for embeddings and `gpt-4.1-mini` for answer generation. It is intended as an internal assistant and not as a final decision-making tool.',\n",
       "   'score': 0.007650701243728815}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c68909-746d-4a1d-b8f8-5f4ed9fe4f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag-eval)",
   "language": "python",
   "name": "rag-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
