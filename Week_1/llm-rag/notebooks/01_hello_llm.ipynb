{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb0a8638-355d-4e12-8ac7-8bc5ba85fab9",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1579fc0f-e713-45f3-963c-7756932c93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "# 1) Load environment variables from .env (OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY is not set. \"\n",
    "        \"Create a .env file in the project root and put your API key there.\"\n",
    "    )\n",
    "\n",
    "# 2) Initialize the OpenAI client (it will read OPENAI_API_KEY automatically)\n",
    "client = OpenAI()\n",
    "\n",
    "# 3) Choose the model\n",
    "MODEL_NAME = \"gpt-4.1-mini\"\n",
    "\n",
    "# 4) Prepare tokenizer for local token counting\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55845a59-37ae-42f9-aef0-cb8707c14253",
   "metadata": {},
   "source": [
    "**LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4edb724f-7db3-493e-9e2e-5c76f0d47a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL ANSWER ===\n",
      "\n",
      "A Large Language Model (LLM) is a type of artificial intelligence designed to understand and generate human-like text by analyzing vast amounts of written data. It uses a neural network with many layers, called a transformer, which processes words in context to predict the next word in a sentence. Engineers often train these models on diverse text sources so they can perform tasks like translation, summarization, or answering questions. The \"large\" part means the model has millions or billions of parameters, enabling it to capture complex patterns in language. In essence, an LLM is a sophisticated pattern recognition system tailored for natural language processing.\n",
      "\n",
      "=== TOKEN USAGE (FROM API) ===\n",
      "Input tokens : 39\n",
      "Output tokens: 124\n",
      "Total tokens : 163\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Explain what a Large Language Model (LLM) is for an engineering student. \"\n",
    "    \"Use 4â€“5 sentences. Use clear language, but be technically correct.\"\n",
    ")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    input=prompt,\n",
    "    max_output_tokens=250,\n",
    ")\n",
    "\n",
    "print(\"=== MODEL ANSWER ===\\n\")\n",
    "print(response.output_text)\n",
    "\n",
    "print(\"\\n=== TOKEN USAGE (FROM API) ===\")\n",
    "print(\"Input tokens :\", response.usage.input_tokens)\n",
    "print(\"Output tokens:\", response.usage.output_tokens)\n",
    "print(\"Total tokens :\", response.usage.total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48b3bd-c052-499c-bd96-254a277c4a91",
   "metadata": {},
   "source": [
    "**Local token counting (tiktoken)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afbbfef-4874-4c22-a477-740b7ae19261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOCAL TOKEN COUNT EXAMPLE ===\n",
      "Text       : Cubic Engineering Consultancy provides end-to-end design and supervision services across multiple regions in the UAE.\n",
      "Token count: 19\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in a given text using the tiktoken encoding.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Cubic Engineering Consultancy provides end-to-end design and \"\n",
    "    \"supervision services across multiple regions in the UAE.\"\n",
    ")\n",
    "\n",
    "print(\"=== LOCAL TOKEN COUNT EXAMPLE ===\")\n",
    "print(\"Text       :\", sample_text)\n",
    "print(\"Token count:\", count_tokens(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe205bf4-37c8-4757-8643-79939a73f5b7",
   "metadata": {},
   "source": [
    "**Cost estimation example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d8f6fa-1466-4e31-8402-0f1ed92c7f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COST ESTIMATE FOR THIS CALL (EXAMPLE) ===\n",
      "Input tokens : 39\n",
      "Output tokens: 124\n",
      "Estimated cost: $0.00021400 USD\n"
     ]
    }
   ],
   "source": [
    "INPUT_PRICE_PER_MILLION = 0.40\n",
    "OUTPUT_PRICE_PER_MILLION = 1.60\n",
    "\n",
    "\n",
    "def estimate_cost_usd(input_tokens: int, output_tokens: int) -> float:\n",
    "    \"\"\"\n",
    "    Estimate cost of a single LLM call, in USD.\n",
    "    \"\"\"\n",
    "    input_cost = (input_tokens / 1_000_000) * INPUT_PRICE_PER_MILLION\n",
    "    output_cost = (output_tokens / 1_000_000) * OUTPUT_PRICE_PER_MILLION\n",
    "    return input_cost + output_cost\n",
    "\n",
    "\n",
    "input_tokens = response.usage.input_tokens\n",
    "output_tokens = response.usage.output_tokens\n",
    "\n",
    "estimated_cost = estimate_cost_usd(input_tokens, output_tokens)\n",
    "\n",
    "print(\"=== COST ESTIMATE FOR THIS CALL (EXAMPLE) ===\")\n",
    "print(\"Input tokens :\", input_tokens)\n",
    "print(\"Output tokens:\", output_tokens)\n",
    "print(f\"Estimated cost: ${estimated_cost:.8f} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8db05-9fb1-4252-b3ab-3b6c7d3b1cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
